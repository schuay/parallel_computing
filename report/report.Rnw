\documentclass[a4paper,10pt]{article}

\usepackage[usenames,dvipsnames]{color}
\usepackage{comment}
\usepackage{listings}
\usepackage{amssymb}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[utf8]{inputenc}

\lstset{
    language=C,                             % Code langugage
    basicstyle=\ttfamily,                   % Code font, Examples:
                                            % \footnotesize, \ttfamily
    keywordstyle=\color{OliveGreen},        % Keywords font ('*' = uppercase)
    commentstyle=\color{gray},              % Comments font
    captionpos=b,                           % Caption-position = bottom
    breaklines=true,                        % Automatic line breaking?
    breakatwhitespace=false,                % Automatic breaks only at
                                            % whitespace?
    showspaces=false,                       % Dont make spaces visible
    showtabs=false,                         % Dont make tabs visible
    morekeywords={__attribute__},           % Specific keywords
}

\title{
    Project Report \\
    Introduction to Parallel Computing, WS 2012
}

\author{
    Jakob Gruber, 0203440 \\
    Mino Sharkhawy, 1025887
}

\begin{comment}
A useful Sweave tutorial can be found at http://users.stat.umn.edu/~geyer/Sweave/.

Short report, 1-3 pages (depending) per project plus
performance plots (1-5 pages). Be ready to discuss this at
presentation, also program code

Be concise, clear, brief:
•What you have done
•How you tested (main test cases, problems encountered)
•What you have not done (assumption like: „the program
assumes p is even“, „n must be a power of two“, ...)
•Be honest – things that don‘t work
•What you intend to show with the experiments

Note will be based on presentation/discussion, and hand-in.

Criteria:
•Correctness, by argument (e.g. merging, prefix-sums), and test
•Well chosen test cases, in principle exhaustive, show that you
have thought about what needs to be tested
•Program actually working, given stated restrictions
•Good plots/tables showing the properties (speed-up, work) of
the implementations
•Achieved performance improvement – don‘t be too depressed if
speed-up is modest and less than p
\end{comment}

\begin{document}

\maketitle

\clearpage
\tableofcontents

% =============================================================================
\clearpage
\section{Introduction}
% =============================================================================

This report presents our solutions to six parallel computing programming projects:
prefix sums and work-optimal merge with OpenMP, a comparison of task-parallel
and data-parallel merge algorithms in Cilk, and parallel stencil, bucket sort
and inclusive scan algorithms using OpenMPI.

The basic structure of each project is roughly the same. It consists of

\begin{itemize}
\item a unit test suite which runs a preset unit test suite,
\item a benchmark executable which times the execution time with specific
      parameters, and
\item optionally a sequential reference implementation.
\end{itemize}

These components use pseudo-random number sequences to generate the test data.
The seed for \lstinline|srand| is specified as a command line parameter for
reproducability. Test cases verify parallel algorithms against a sequential
reference implementation, the results of which are either produced on-the-fly
or persisted on disk.

The code of all projects is available at our github
repository\footnote{\url{https://github.com/schuay/parallel_computing}}.
% TODO: Make this public.

In addition to the standard C and Unix toolchain (GCC, make, etc.) as well as
special tools required by Cilk and OpenMPI, we also used
CMake\footnote{\url{http://www.cmake.org/}} to manage our builds and
Check\footnote{\url{http://check.sourceforge.net/}} for unit testing.

Check is a simple unit testing framework for C, featuring a simple interface
for defining unit tests. We ran into some trouble while testing OpenMPI
programs with check: by default it forks each test into a separate process,
which eventually leads to a segmentation fault at the latest when
\lstinline|MPI_Finalize| is called. Fortunately, we were able to work around
this issue by setting the \lstinline|CK_FORK| environment variable to ,,no'',
which disables forking during test suite runs.

% =============================================================================
\clearpage
\section{OpenMP}
% =============================================================================

\subsection{Parallel Prefix Sums}

\subsubsection{Specification}

Implement the 3 parallel prefix sums algorithms from the lecture:

\begin{enumerate}
\item Recursive parallel prefix with auxiliary array $y$
\item In-place iterative algorithm
\item $O(n \log n)$ work algorithm (Hillis-Steele)
\end{enumerate}

All algorithms shall work on arrays of some basetype given at compile time
(int, double, ...) with the ``+'' operator.
Implement non-intrusive ``performance counters'' for documenting that the work
is indeed $O(n)$ and $O(n \log n)$.
The implementations shall be correct for all array sizes $n$, and any number of
threads, $1,...,max$.
Test and benchmark the implementations, for OpenMP compare performance to
``reduction'' clause.

\subsubsection{Implementation}

% TODO

A few of these algorithms may also be simplified to compute only the total
sum instead of all prefix sums:

\begin{itemize}
\item The work performed by the iterative algorithm can be cut in half
      by skipping the entire down phase, which is only responsible for
      distributing results into the entire data range.
\item It does not seem to be possible to simplify the Hillis-Steele
      algorithm to compute only the final sum. % TODO: Is this correct?
% TODO: Recursive
\end{itemize}


\subsubsection{Testing}

Our unit tests compare the sequential results against our parallel
algorithms. Test cases include:

\begin{itemize}
\item an input array with an odd number of elements
\item an input array with an even number of elements
\item an input array containing only one element
\item an input array containing containing $1024$ random elements ($seed = 1024$)
\item an input array containing containing $16384$ random elements ($seed = 16384$)
\end{itemize}

\subsubsection{Results}

% TODO
% TODO Verify complexity with performance counters
% TODO Bonus: Can the algorithms be simplified to compute only the total sum?

<<results=tex,echo=FALSE>>=
\SweaveInput{functions.Rnw}
readAndPlot("../1-prefix_sums/data/bench.csv")
@

\clearpage
\subsection{A Work-Optimal Merge Algorithm}

\subsubsection{Specification}

Implement a work-optimal merge algorithm for merging two
sorted arrays of size $n$ and $m$ in $O(\frac{m+n}{p}+\log n +\log m)$ steps.
The implementation shall work for all $n$ and $m$, any C base
datatype, but may assume that elements in the two array are all
different.

Implement either:

\begin{enumerate}
\item binary search from block starts of array $a$ into array $b$. Merge (in
parallel) all pairs no larger than $\frac{m+n}{p}$; handle larger pairs by binary
search from array $b$ to $a$
\item binary search from block starts of $a$ and block starts of $b$. Describe
briefly the special cases for the binary search for locating subarrays, and how
this leads to all sub- merge problems having size $O(\frac{n}{p}+\frac{m}{p})$
\end{enumerate}

Argue for correctness by testing. Benchmark and compare to standard, sequential
merge implementation from lecture (or better one, if known), report speed-up.

\subsubsection{Implementation}

For our parallel merge implementation we chose to go with the first alternative,
in which $a$ is divided into blocks (and $b$ only if a thread's range is larger
than $\frac{m+n}{p}$).

The first parallelization occurs at the beginning of the algorithm, when
$a$ is split into partitions which are assigned to a thread each. If
$n < p$, $a$ is split into $n$ partitions instead.

Each thread then proceeds to use a sequential binary search to determine the
beginning and end of its partition when merged into $b$. In case the
combined length of its $a$ and $b$ partitions exceeds $\frac{m+n}{p}$,
the partition function is called recursively with the arguments of
$a$ and $b$ reversed\footnote{Note that this kind of nested parallelism requires setting
\lstinline|omp_set_nested(1)| since it is disabled by default.}.

On conclusion of the partition function, its assigned ranges are merged with
the references sequential merge implementation.

\subsubsection{Testing}

Our merge unit tests compare the sequential results against our parallel
algorithm. Test cases include:

\begin{itemize}
\item $m, n = 1$, $a_0 < b_0$
\item $m, n = 1$, $a_0 > b_0$
\item $\forall i, j: a_i < b_j$
\item $\forall i, j: a_i > b_j$
\item fully interleaved ranges $a$ and $b$
\item blockwise interleaved ranges $a$ and $b$
\item large interleaved ranges $a$ and $b$ with $n = 500000$
\item large pseudo-random ranges with $n = 500000$ (repeatable by
      using a predetermined seed)
\end{itemize}

\subsubsection{Results}

% TODO: Bonus: Can the algorithm be extended to allow element repetitions?
%       Which properties can be guaranteed?
% TODO: Bonus: Can the algorithm be used for implementing a parallel mergesort?
%       What is missing?

<<results=tex,echo=FALSE>>=
readAndPlot("../1-merge/data/bench.csv")
@

% =============================================================================
\clearpage
\section{Cilk}
% =============================================================================

\subsection{Task-Parallel vs. Data-Parallel Merge Algorithms}

\subsubsection{Specification}

Implement the recursive, task parallel merge algorithm from the
lecture.

Compare to an implementation of the binary search based
algorithm from the lecture (also implemented in Cilk); use the
data parallel construct to execute the desired number of binary
searches in parallel.

Comment on ease of implementation, and achieved performance.


\subsubsection{Implementation}

In the task parallel implementation $a$ is split in half and then $b$ is split into two
parts, where the first one only contains elements smaller than those in the larger half
of $a$ and the second part the rest. To find the correct spot where $b$ has to be split,
a binary search is used.

This process is then repeated for parts of $a$ and $b$ containing the smaller elements
and the parts containing the larger ones. Since there are no dependencies between them,
this step is done in parallel, by recursively spawning \lstinline|merge_recursive()|.

Once $m + n$ is $\leq unit$ the recursion stops and the remaining problem is solved
sequentially. We use $\frac{m + n}{p}$ as our $unit$.\\

The data parallel algorithm first uses the data parallel construct to split $a$ in $p$
parts and perform a binary search for each block of $a$ to determine it's rank in $b$.
These searches are performed in parallel.

Once each rank has been determined, the ranks are used to determine for each segment
in $a$ it's according segment in $b$. If both segments together are $\leq \frac{m + n}{p}$,
they are merged using a sequential merge algorithm. This again happens in parallel for
each segment in $a$.

If the parts to merge remain $> \frac{m + n}{p}$, the data-parallel algorithm is performed
again on them. Since the segment of $a$ already has a size of $n/p$, the segment of $b$ is
split this time.\\

Implementation-wise the recursive algorithm is slightly simpler than the data parallel
one. This is mostly due to the fact, that the parallelization is far more explicit in
the data parallel algorithm. Additionally, the use of the data parallel construct adds
complexity in the form of function pointers and large argument structs.

On the other hand, the recursive algorithm may require tuning the $unit$ value to achieve
optimal performance, whereas the data is already partitioned according to the number of
processors in the data parallel one.

\subsubsection{Testing}

Our merge unit tests compare the sequential results against either parallel
algorithm. Test cases include:

\begin{itemize}
\item $m, n = 1$, $a_0 < b_0$
\item $m, n = 1$, $a_0 > b_0$
\item $\forall i, j: a_i < b_j$
\item $\forall i, j: a_i > b_j$
\item fully interleaved ranges $a$ and $b$
\item blockwise interleaved ranges $a$ and $b$
\item interleaving at the end of $a$ and the beginning of $b$
\item large interleaved ranges $a$ and $b$ with $n = 500000$
\item large pseudo-random ranges with $n = 500000$ (repeatable by
      using a predetermined seed)
\end{itemize}

\subsubsection{Results}

The sequential reference and both parallel algorithms were benchmarked with $10$, $100$,
$1000000$, $1111111$ and $100000000$ as input size, with $n = \lfloor size / 2\rfloor$ and
$m = size - n$.

The parallel algorithms were benchmarked with $8$, $16$, $32$, $33$ and $48$ threads for each
input size.\\

Both the data parallel and the recursive algorithm perform almost equally well (or bad) with the
latter being slightly faster than the former. At an input size of $100000000$ this relation is
reversed.

It is also interesting to note, that the improvement over the sequential algorithm is minimal
before the $100000000$ benchmark. Additionally, at lower input sizes the threading overhead
is so large, that increasing the number of parallel threads actually slows down execution.

<<results=tex,echo=FALSE>>=
readAndPlot("../2-merge/data/bench.csv")
@

% =============================================================================
\clearpage
\section{MPI}
% =============================================================================

\subsection{Parallel Stencil Computation}

\subsubsection{Specification}

Let A be a 2-dimensional $n \times m$ matrix, that is to be distributed
over the $p$ MPI processes. Implement a distributed version of
the two-dimensional stencil-computation: each MPI process has
a submatrix, and therefore needs to exchange (in each
iteration?) only the elements on the border of the submatrix
with its neighboring processes

\begin{itemize}
\item Give a ,,generic'' implementation that allows distributions of $A$
in rows, in columns or in $n/c \times m/r$ blocks, where $cr = p$.
\item Implement the border exchange with MPI\_Sendrecv
communication (other implementations allowed!); beware of
deadlock.
\item Try to give an ,,analytical'' estimate for the running time,
depending on $n$ and $m$, $c$ and $r$, and the number of MPI
processes. It can be assumed that communication time is
dependent as $O(k)$ on the size of the data being communicated.
\item What is the matrix distribution giving the best performance
under this model?
\item Benchmark the implementation, test for correctness (compare
to sequential solution, e.g.). Does the measured performance
correspond to the expectations derived from the model?
\end{itemize}

\subsubsection{Implementation}

The stencil algorithm is implemented as a function with
a matrix, an iteration count, and the matrix partitioning specified
as $r$ and $c$ as arguments. We assume that $r \cdot c = p$,
$r \cdot x = m$ and $c \cdot y = n$ for some $x, y \in \mathbb{N}$.

Our opaque \lstinline|matrix_t| type is implemented as a struct with various
utility functions for tasks such as accessing its elements, retrieving
its dimensions, comparing matrices, saving and loading from files, and
extracting as well as integrating submatrices.

The sequential reference implementation (a simple nested loop which
copies $B_{i,j} = (A_{i-1, j} + A_{i+1, j} + A_{i, j-1}, A_{i, j+1}) / 4$
to a temporary matrix $B$ which is subsequently copied back to $A$ after
each iteration) uses \lstinline|matrix_t| heavily. Results are saved to
files for use in test verification.

The parallel algorithm on the other hand works mostly with raw \lstinline|double|
arrays. These are extracted from the main matrix with \lstinline|matrix_extract|
and reintegrated with \lstinline|matrix_cram|. The \lstinline|submatrix_t| type
is public, and as such each process is able to extract the required element array.

The idea of the parallel stencil itself is very simple and consists only of two
steps:

\begin{enumerate}
\item Processes are arranged in a 2D grid which have corresponding $m/r \times n/c$
      submatrices. In this phase, each process exchanges its border elements with
      neighbor matrices; the top row with the top neighbor, etc.
\item Each process now has all needed information to calculate the stencil of its
      submatrix, which is then done sequentially.
\end{enumerate}

These steps are repeated for each iteration. Communications through the 2D grid
are simplified by the use of MPI cartesian topologies.
Upon conclusion, the main matrix is reformed after gathering all submatrices.

The complexity of the parallel algorithm is
$O(i \cdot (\frac{mn}{rc} + \frac{m}{r} + \frac{n}{c}) + \log p + mn)$.
The first additive term stems from the iteration calculation,
the second and third from the subsequent combination of results.
Leaving the recombination out and limiting ourselves to a single
iteration, the complexity reduces to $O(\frac{mn}{rc} + \frac{m}{r} + \frac{n}{c})$.

According to this analysis, $r$ and $c$ should be chosen such that
$\frac{m}{r} + \frac{n}{c}$ is minimal.

\subsubsection{Testing}

As mentioned above, correctness of the parallel stencil algorithm is established
by comparison against persisted results of the sequential reference implementation.

Unit tests include:

\begin{itemize}
\item a $1000 \times 1000$ matrix distributed by columns at 100, 300, and 500 iterations
\item a $1000 \times 1000$ matrix distributed by rows at 100, 300, and 500 iterations
\item a $1000 \times 1000$ matrix distributed blockwise at 100, 300, and 500 iterations
\end{itemize}

\subsubsection{Results}

In order to execute our benchmark on all available nodes, we had to add
\lstinline|export PATH=/opt/openmpi/bin:\$PATH| to all local \lstinline|~/.bashrc| files.

As expected, the parallel stencil implementation scales very well: with recombination
(in which all processes end up with the full matrix upon conclusion), we were able to
achieve super-linear speedup of $112.48$ for $1000$ stencil iterations performed on
a $1000 \times 1000$ matrix with $80$ processes.

The optimal matrix partitioning also fits our model, with the $200 \times 125$ submatrices
outperforming the other partitions ($500 \times 50$ and $1000 \times 25$) by almost a factor
of $2$.

<<results=tex,echo=FALSE>>=
readAndPlot("../3-stencil/data/bench.csv")
@


\clearpage
\subsection{Parallel Integer Bucket Sort}

\subsubsection{Specification}

Implement the integer bucket sort algorithm from the lecture.
Assume an array of integers in a given range $[0,R-1]$ distributed
in roughly equal sized blocks over the MPI processes.
The algorithm uses MPI\_Allreduce and MPI\_Exscan to compute
the size of the buckets and to make it possible to determine for
each array element its position in the sorted output. The
(implementation) difficulty is to use this information to set up
an MPI\_Alltoallv operation to perform collectively the
redistribution of the array elements into sorted order.

\begin{itemize}
\item Show correctness by testing.
\item Benchmark and show speed-up/decrease in run-time for
different array sizes $n$ (with $m = \frac{n}{p}$ elements per MPI process)
and different ranges $R$. Is there a ,,best'' choice of R? Argue
also theoretically, assuming that MPI\_Exscan and
MPI\_Allreduce both have complexity O($m+\log p$).
\item Bonus: use the bucket sort algorithm to implement a full-
fledged radix sort. How should the radix be chosen?
\end{itemize}

\subsubsection{Implementation}

Bucket sort takes an array of integers $xs$ of length $n$, with all elements
$x \in xs$ having a value less than an upper bound $R$. It is executed in parallel
on $p$ processes.
The algorithm as implemented differs from the specification in the lecture
slides. The steps are as follows:

\begin{enumerate}
\item Each process is assigned an initial index range of $xs$ and a bucket value range.
\item The goal of this first phase is to end up with $x \in xs$ distributed such across all
      processes such that each owned element falls within the processes bucket
      value range. First, the owned range of $xs$ is sorted sequentially.
\item Then, each process determines the range of elements to send to other processes, and
\item exchanges the elements.
\item Finally, the local elements are again sequentially sorted.
\item As an optional step, the sorted buckets can be recombined.
\end{enumerate}

Our implementation uses \lstinline|MPI_Alltoall| and \lstinline|MPI_Alltoallv|
to redistribute the array elements, and \lstinline|MPI_Allgather| and
\lstinline|MPI_Allgatherv| to recombine all sorted buckets.

The complexity of our implementation is $O(\frac{n}{p} \log \frac{n}{p} + p + n
+ \frac{n}{p} \log p)$ with
the optional recombination, and $O(\frac{n}{p} \log \frac{n}{p} + p)$ without it.

The value of $R$ is not important as long as $R \geq p$ and the elements $x \in xs$
are distributed such that each bucket is roughly equal in size.

\subsubsection{Testing}

The parallel bucket sort algorithm is verified by comparing results against the
sequential C \lstinline|qsort| function. The test suite includes:

\begin{itemize}
\item a sorted 2-element sequence
\item an unsorted 2-element sequence
\item a sequence with an odd number of elements
\item a sequence with a a range of smaller elements followed by larger elements
\item a sequence with a a range of larger elements followed by smaller elements
\item a sequence consisting of two interleaved ranges
\item a sequence consisting of several interleaved ranges
\item a large pseudorandom sequence with $n = 500000$ and $R = 400000$
\end{itemize}

We initially ran into minor issues with test cases in which the process count $p$
was greater than the size of the sequence $n$. This problem was circumvented
by requiring $p \leq n$. Another possible solution would have been to create a new
\lstinline|MPI_Comm| containing a suitable subset of processes.

\subsubsection{Results}

This bucket sort implementation only begins showing speed-up at rather high data amounts.
We investiged this phenomenon, and came to the conclusion that nearly all time was spent
in the calls to \lstinline|MPI_Alltoall| and \lstinline|MPI_Alltoallv| calls, which we
cannot influence.

<<results=tex,echo=FALSE>>=
readAndPlot("../3-bucket_sort/data/bench.csv")
@

% TODO: Bonus radix sort

\clearpage
\subsection{Inclusive Scan}

\subsubsection{Specification}

Given an array $A$ distributed blockwise over the $p$ MPI
processes. Implement an algorithm (see 2nd lecture!) for
computing all inclusive prefix-sums of $A$: The function
\lstinline|arrayscan(int A[], int n, MPI_Comm comm)|
shall compute for MPI rank $i$ in $A[j]$ the sum
$(\forall i' < i: \sum_{k=0}^{k<ni'} A[k])+A[0]+...+A[k]$.

Operation is integer sum ,,+''; but only associativity should be
exploited for the parallelization. Note that the processes may
contribute blocks of A of different sizes.

Implementation hint: compute prefix sums of blocks locally, use a
scan algorithm (as in lecture; e.g. 3rd algorithm) to compute all
prefix sums of local sums, add prefix locally:

\begin{enumerate}
\item Locally compute \lstinline|Scan(A,n)|, let $B=a[n-1]$ for each process
\item Do distributed \lstinline|ExScan'(B)| to compute for rank $i$ the sum
      $B(0)+B(1)+...+B(i-1)$
\item  Locally compute for rank $i$: $A[j] = A[j]+B(i-1)$ for $0 \leq j < n$
\end{enumerate}

Step 2 is the crucial step and requires MPI communication. This
can be done by \lstinline|MPI_Exscan| - but the task of the project is to
implement an own algorithm (hint: Hillis-Steele)

\begin{itemize}
\item Establish correctness by comparing to sequential scan
\item What is the asymptotic running time of your algorithm as a
      function of $n$ and $p$? Which assumptions do you need for the
      estimate?
\item Compute speed-up relative to sequential Scan for different
      (large) $n \in 100000, 1000000, 10000000, ...$
\item How is this function different from MPI's scan?
\end{itemize}

\subsubsection{Implementation}

The parallel scan first splits the input array into $p$ segments, where the
segment for each process starts at $offset = rank * n / p$ and contains
$length = n / p$ elements (unless $rank = p - 1$, in which case $size = n - offset$).

Next each process computes the inclusive prefix-sum array for its segment. The last
element of this array ($b$) is then contributed to the distributed Exscan. We can
either use MPI's Exscan or our own implementation, which is detailed below.

The Exscan yields the (exclusive) prefix for each process' rank, which is then added
to each element of the locally computed prefix-sum array.

Lastly, all of the locally computed prefix-sums are sent to the ''Master`` process
($rank = 0$) using \lstinline|MPI_Gatherv()|. At first we used \lstinline|MPI_Alltoallv()|
and distributed the result to all processes, but this involved a larger communication
overhead.\\

Our Exscan implementation uses the Hillis-Steele algorithm from the lecture. However,
each process executes one iteration of the inner loops. In each iteration of the outer
loop a process with $rank$ will send its current contribution $xi$ to the process with
$rank + k$ if it exists. It will also receive from the process with $rank - k$ and add
the received value to its last contribution.

The outer loop starts with $k = 1$ and $xi$ as the last element of the local prefix
calculation (see above). It runs until $k \geq p$, where $k$ is doubled after each
iteration.

After this, each process holds the inclusive prefix-sum for the first element of its
local segment. Since we need the exclusive one and are not allowed to substract our
initial contribution (we may only assume associativity, but not the existence of an
inverse), each process sends its sum to the process with $rank + 1$ and returns the
received value (or $0$ for the process with $rank = 0$) to the Arrayscan
implementation.\\

This Hillis-Steele Exscan has an asymptotic running time of $O(\log p)$ assuming that
$p$ is a power of $2$. In addition to that, the local array sums take $O(\frac{2 * n}{p})$
and the final Gather takes $O(\frac{n}{p} + \log p)$. So the asymptotic running time of the
entire Arrayscan algorithm is $O(\frac{3 * n}{p} + 2 * \log p)$. Assuming, that we don't
have to collect the the data it can be cut down to $O(\frac{2 * n}{p} + \log p)$.

\subsubsection{Correctness}
% TODO

\subsubsection{Testing}

Our unit tests compare the sequential results against our parallel
algorithms. Test cases include:

\begin{itemize}
\item an input array with an odd number of elements
\item an input array with an even number of elements
\item an input array containing only one element
\item an input array containing containing $1024$ random elements ($seed = 1024$)
\item an input array containing containing $16384$ random elements ($seed = 16384$)
\end{itemize}

\subsubsection{Results}

<<results=tex,echo=FALSE>>=
readAndPlot("../3-inclusive_scan/data/bench.csv")
@

\end{document}
