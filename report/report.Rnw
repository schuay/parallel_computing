\documentclass[a4paper,10pt]{article}

\usepackage{comment}
\usepackage{listings}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[utf8]{inputenc}

\lstset{
    language=C,                             % Code langugage
    basicstyle=\ttfamily,                   % Code font, Examples:
                                            % \footnotesize, \ttfamily
    keywordstyle=\color{OliveGreen},        % Keywords font ('*' = uppercase)
    commentstyle=\color{gray},              % Comments font
    captionpos=b,                           % Caption-position = bottom
    breaklines=true,                        % Automatic line breaking?
    breakatwhitespace=false,                % Automatic breaks only at
                                            % whitespace?
    showspaces=false,                       % Dont make spaces visible
    showtabs=false,                         % Dont make tabs visible
    morekeywords={__attribute__},           % Specific keywords
}

\title{Introduction to Parallel Computing, WS 2012}
\author{
    Jakob Gruber, 0203440 \\
    Mino Sharkhawy, 1025887
}

\begin{comment}
A useful Sweave tutorial can be found at http://users.stat.umn.edu/~geyer/Sweave/.

Short report, 1-3 pages (depending) per project plus
performance plots (1-5 pages). Be ready to discuss this at
presentation, also program code

Be concise, clear, brief:
•What you have done
•How you tested (main test cases, problems encountered)
•What you have not done (assumption like: „the program
assumes p is even“, „n must be a power of two“, ...)
•Be honest – things that don‘t work
•What you intend to show with the experiments

Note will be based on presentation/discussion, and hand-in.

Criteria:
•Correctness, by argument (e.g. merging, prefix-sums), and test
•Well chosen test cases, in principle exhaustive, show that you
have thought about what needs to be tested
•Program actually working, given stated restrictions
•Good plots/tables showing the properties (speed-up, work) of
the implementations
•Achieved performance improvement – don‘t be too depressed if
speed-up is modest and less than p
\end{comment}

\begin{document}

\maketitle

\clearpage
\tableofcontents

\clearpage
\section{Tools}

% TODO: cmake, make, check

\clearpage
\section{OpenMP}

\subsection{Parallel Prefix Sums}

\subsubsection{Specification}

Implement the 3 parallel prefix sums algorithms from the lecture:

\begin{enumerate}
\item Recursive parallel prefix with auxiliary array $y$
\item In-place iterative algorithm
\item $O(n \log n)$ work algorithm (Hillis-Steele)
\end{enumerate}

All algorithms shall work on arrays of some basetype given at compile time
(int, double, ...) with the ``+'' operator.
Implement non-intrusive ``performance counters'' for documenting that the work
is indeed $O(n)$ and $O(n \log n)$.
The implementations shall be correct for all array sizes $n$, and any number of
threads, $1,...,max$.
Test and benchmark the implementations, for OpenMP compare performance to
``reduction'' clause.

\subsubsection{Results}

<<results=tex,echo=FALSE>>=
\SweaveInput{functions.Rnw}
readAndPlot("../1-prefix_sums/data/bench.csv")
@

\clearpage
\subsection{A Work-Optimal Merge Algorithm}

\subsubsection{Specification}

Implement a work-optimal merge algorithm for merging two
sorted arrays of size $n$ and $m$ in $O((m+n)/p+\log n +\log m)$ steps.
The implementation shall work for all $n$ and $m$, any C base
datatype, but may assume that elements in the two array are all
different.

Implement either:

\begin{enumerate}
\item binary search from block starts of array $a$ into array $b$. Merge (in
parallel) all pairs no larger than $(m+n)/p$; handle larger pairs by binary
search from array $b$ to $a$
\item binary search from block starts of $a$ and block starts of $b$. Describe
briefly the special cases for the binary search for locating subarrays, and how
this leads to all sub- merge problems having size $O(n/p+m/p)$
\end{enumerate}

Argue for correctness by testing. Benchmark and compare to standard, sequential
merge implementation from lecture (or better one, if known), report speed-up.

\subsubsection{Results}

<<results=tex,echo=FALSE>>=
readAndPlot("../1-merge/data/bench.csv")
@

\clearpage
\section{Cilk}

\subsection{Task-Parallel vs. Data-Parallel Merge Algorithms}

\clearpage
\section{MPI}

% TODO: Mention testing issues with MPI -> CK_FORK=no

\subsection{Parallel Stencil Computation}

\subsubsection{Specification}

Let A be a 2-dimensional $n$x$m$ matrix, that is to be distributed
over the $p$ MPI processes. Implement a distributed version of
the two-dimensional stencil-computation: each MPI process has
a submatrix, and therefore needs to exchange (in each
iteration?) only the elements on the border of the submatrix
with its neighboring processes

\begin{itemize}
\item Give a ,,generic'' implementation that allows distributions of $A$
in rows, in columns or in $n/c$ x $m/r$ blocks, where $cr = p$.
\item Implement the border exchange with MPI\_Sendrecv
communication (other implementations allowed!); beware of
deadlock
\item Try to give an ,,analytical'' estimate for the running time,
depending on $n$ and $m$, $c$ and $r$, and the number of MPI
processes. It can be assumed that communication time is
dependent as $O(k)$ on the size of the data being communicated.
\item What is the matrix distribution giving the best performance
under this model?
\item Benchmark the implementation, test for correctness (compare
to sequential solution, e.g.) . Does the measured performance
correspond to the expectations derived from the model?
\end{itemize}

\clearpage
\subsection{Parallel Integer Bucket Sort}

\subsubsection{Specification}

Implement the integer bucket sort algorithm from the lecture.
Assume an array of integers in a given range $[0,R-1]$ distributed
in roughly equal sized blocks over the MPI processes.
The algorithm uses MPI\_Allreduce and MPI\_Exscan to compute
the size of the buckets and to make it possible to determine for
each array element its position in the sorted output. The
(implementation) difficulty is to use this information to set up
an MPI\_Alltoallv operation to perform collectively the
redistribution of the array elements into sorted order

\begin{itemize}
\item Show correctness by testing
\item Benchmark and show speed-up/decrease in run-time for
different array sizes $n$ (with $m = n/p$ elements per MPI process)
and different ranges $R$. Is there a ,,best'' choice of R? Argue
also theoretically, assuming that MPI\_Exscan and
MPI\_Allreduce both have complexity O($m+\log p$)
\item Bonus: use the bucket sort algorithm to implement a full-
fledged radix sort. How should the radix be chosen?
\end{itemize}

\subsubsection{Implementation}

Bucket sort takes an array of integers $xs$ of length $n$, with all elements
$x \in xs$ having a value less than an upper bound $u$. It is executed in parallel
on $p$ processes.
The algorithm as implemented differs from the specification in the lecture
slides. The steps are as follows:

\begin{enumerate}
\item Each process is assigned an initial index range of $xs$ and a bucket value range.
\item The goal of this first phase is to end up with $x \in xs$ distributed such across all
      processes such that each owned element falls within the processes bucket
      value range. First, the owned range of $xs$ is sorted sequentially.
\item Then, each process determines the range of elements to send to other processes, and
\item exchanges the elements.
\item Finally, the local elements are again sequentially sorted.
\item As an optional step, the sorted buckets can be recombined.
\end{enumerate}

Our implementation uses \lstinline|MPI_Alltoall| and \lstinline|MPI_Alltoallv|
to redistribute the array elements, and \lstinline|MPI_Allgather| and
\lstinline|MPI_Allgatherv| to recombine all sorted buckets.

The complexity of our implementation is $O(n/p \log (n/p) + p + n/p \log p)$ with
the optional recombination, and $O(n/p \log (n/p) + p)$ without it.

The value of $u$ is not important as long as $u >= p$ and the elements $x \in xs$
are distributed such that each bucket is roughly equal in size.

\subsubsection{Testing}

The parallel bucket sort algorithm is verified by comparing results against the
sequential C \lstinline|qsort| function. The test suite includes:

\begin{itemize}
\item a sorted 2-element sequence
\item an unsorted 2-element sequence
\item a sequence with an odd number of elements
\item a sequence with a a range of smaller elements followed by larger elements
\item a sequence with a a range of larger elements followed by smaller elements
\item a sequence consisting of two interleaved ranges
\item a sequence consisting of several interleaved ranges
\item a large pseudorandom sequence with $n = 500000$ and $u = 400000$
\end{itemize}

We initially ran into minor issues with test cases in which the process count $p$
was greater than the size of the sequence $n$. This problem was circumvented
by requiring $p \leq n$. Another possible solution would have been to create a new
\lstinline|MPI_Comm| containing a suitable subset of processes.

\subsubsection{Results}

% TODO: Results
% TODO: Bonus radix sort

\clearpage
\subsection{...}

\end{document}
